{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31a792bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"True\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85c7129a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Data Ingestion\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "128ac3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x2801e2bd550>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://blog.langchain.dev/why-langgraph-platform/\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f32faca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://blog.langchain.dev/why-langgraph-platform/', 'title': 'Why do I need LangGraph Platform for agent deployment?', 'description': 'This blog dives into technical details for why agent deployment is difficult, and how we built a platform to solve those challenges (LangGraph Platform).', 'language': 'en'}, page_content='\\n\\n\\nWhy do I need LangGraph Platform for agent deployment?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nChangelog\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhy do I need LangGraph Platform for agent deployment?\\nThis blog dives into technical details for why agent deployment is difficult, and how we built a platform to solve those challenges (LangGraph Platform).\\n\\n4 min read\\nMay 22, 2025\\n\\n\\n\\n\\n\\nLast we week, we announced the GA of LangGraph Platform, our deployment platform for long running, stateful, or bursty agents. This blog is a technical explanation of why a deployment platform for agents is needed and how we built it.IntroOver the past year we’ve seen companies like LinkedIn, Uber, Klarna, and Elastic successfully build agents with LangGraph. We increasingly have confidence that LangGraph’s high degree of control, built-in persistence layer, and human-in-the-loop features are the right abstractions for building reliable agents.After an agent is built, it then time to deploy.For some agents - deployment is easy. Specifically, if you have stateless agents that run quickly and at low volumes - then deployment isn’t that hard! You can run them as a lambda and all will be fine.For other types of agents, however, deployment becomes trickier. Specifically, the longer running, more stateful, or more bursty your agent is, the more complicated you deployment will be.Long Running AgentsProblemFor agents that take longer to process (e.g., hours), maintaining an open connection can be impractical.How we solve it:The LangGraph Server supports launching agent runs in the background and provides polling endpoints, streaming endpoints and webhooks to monitor run status effectively.ProblemRegular server setups often encounter timeouts or disruptions when handling requests that take a long time to complete.How we solve itLangGraph Server’s API provides robust support for these tasks by sending regular heartbeat signals, preventing unexpected connection closures during prolonged processes. Additionally, stream endpoints can be rejoined if the connection drops, and will very soon buffer events emitted while you were disconnected.ProblemThe longer an agent runs for, the more likely it is to suffer an exception while running.How we solve itLangGraph Server implements strategies to both minimize the number of exceptions agents face (eg. by running them in workers which can use isolated event loops and background threads) as well as the amount of time needed to recover from an exception (eg. when an agent fails it is retried a configurable number of times, and each retry starts off from the most recent successful checkpoint).ProblemThe longer an agent runs for, the more you need intermediate output to be emitted while the agent is running, to show the end user that something is happening.How we solve itLangGraph Server implements multiple streaming modes, including intermediate results, token-by-token LLM messages, all the way to streaming of custom payloads emitted by your nodes. Streaming events are emitted in realtime from the background workers that run the agents, with minimal latency, giving you a unique combo of durable execution with realtime output. Multiple consumers can connect to the output stream of the same agent, and these connections can be re-established if dropped by the client or intermediate networking layers.Bursty AgentsProblemCertain applications, especially those with real-time user interaction, may experience \"bursty\" request loads where numerous requests hit the server simultaneously.How we solve itLangGraph Server includes a task queue, ensuring requests are handled consistently without loss, even under heavy loads. LangGraph Server is designed to scale horizontally on both the server and queue components. This scaling is transparent to you, and doesn’t require complicated load balancing, as all server instances are stateless, and all server instances can communicate with all worker instances to support real-time streaming and cancellation. You can add as many queue instances as needed to handle your desired throughput, and they will all share the queued runs fairly, without ever executing the same run more than once.ProblemSome of types of bursty applications that are also stateful (see below) may let the user send multiple messages rapidly, without the agent having finished responding to the first one. This “double texting” can disrupt agent flows if not handled properly.How we solve itLangGraph Server offers four built-in strategies to address and manage such interactions.Stateful AgentsProblemThere are several levels of statefulness. There is both short-term and long-term memory. Then there is human-in-the-loop (pausing at an arbitrary point in your graph until you get back human input). On top of that, there is human-on-the-loop (time travel) - letting users go back to a previous state of the graph, modify it, and resume from there.There are also different shapes of state. The most simple one is just a list of messages. Most agents, however, have more complicated state than that.The more of these stateful operations you want to support, and the more complex your state, then the more complicated it is to properly set up infrastructure to enable this.How we solve itLangGraph Platform includes optimized\\xa0checkpointers\\xa0and a\\xa0memory store, managing state across sessions without the need for custom solutions.LangGraph Server provides specialized endpoints for human-in-the-loop scenarios, simplifying the integration of manual oversight into agent workflows.ProblemThe more your agent is used, the more memory is used to store conversations and memories that may no longer be relevant.How we solve itLangGraph Server supports attaching TTLs (time-to-live) to both conversation threads as well as long-term memory entries, which will be cleared from memory by the platform when they expire.ConclusionBy using LangGraph Platform, you gain access to a robust, scalable deployment solution that mitigates these challenges, saving you the effort of implementing and maintaining them manually. This allows you to focus more on building effective agent behavior and less on solving deployment infrastructure issues.To get started, read about our deployment options\\xa0here. You can also check out our announcement LangGraph Platform\\'s general availability to learn more.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2025\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baafdab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "final_doc = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7631193b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://blog.langchain.dev/why-langgraph-platform/', 'title': 'Why do I need LangGraph Platform for agent deployment?', 'description': 'This blog dives into technical details for why agent deployment is difficult, and how we built a platform to solve those challenges (LangGraph Platform).', 'language': 'en'}, page_content='Why do I need LangGraph Platform for agent deployment?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nChangelog\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhy do I need LangGraph Platform for agent deployment?\\nThis blog dives into technical details for why agent deployment is difficult, and how we built a platform to solve those challenges (LangGraph Platform).\\n\\n4 min read\\nMay 22, 2025'),\n",
       " Document(metadata={'source': 'https://blog.langchain.dev/why-langgraph-platform/', 'title': 'Why do I need LangGraph Platform for agent deployment?', 'description': 'This blog dives into technical details for why agent deployment is difficult, and how we built a platform to solve those challenges (LangGraph Platform).', 'language': 'en'}, page_content='Last we week, we announced the GA of LangGraph Platform, our deployment platform for long running, stateful, or bursty agents. This blog is a technical explanation of why a deployment platform for agents is needed and how we built it.IntroOver the past year we’ve seen companies like LinkedIn, Uber, Klarna, and Elastic successfully build agents with LangGraph. We increasingly have confidence that LangGraph’s high degree of control, built-in persistence layer, and human-in-the-loop features are the right abstractions for building reliable agents.After an agent is built, it then time to deploy.For some agents - deployment is easy. Specifically, if you have stateless agents that run quickly and at low volumes - then deployment isn’t that hard! You can run them as a lambda and all will be fine.For other types of agents, however, deployment becomes trickier. Specifically, the longer running, more stateful, or more bursty your agent is, the more complicated you deployment will be.Long'),\n",
       " Document(metadata={'source': 'https://blog.langchain.dev/why-langgraph-platform/', 'title': 'Why do I need LangGraph Platform for agent deployment?', 'description': 'This blog dives into technical details for why agent deployment is difficult, and how we built a platform to solve those challenges (LangGraph Platform).', 'language': 'en'}, page_content='fine.For other types of agents, however, deployment becomes trickier. Specifically, the longer running, more stateful, or more bursty your agent is, the more complicated you deployment will be.Long Running AgentsProblemFor agents that take longer to process (e.g., hours), maintaining an open connection can be impractical.How we solve it:The LangGraph Server supports launching agent runs in the background and provides polling endpoints, streaming endpoints and webhooks to monitor run status effectively.ProblemRegular server setups often encounter timeouts or disruptions when handling requests that take a long time to complete.How we solve itLangGraph Server’s API provides robust support for these tasks by sending regular heartbeat signals, preventing unexpected connection closures during prolonged processes. Additionally, stream endpoints can be rejoined if the connection drops, and will very soon buffer events emitted while you were disconnected.ProblemThe longer an agent runs for,'),\n",
       " Document(metadata={'source': 'https://blog.langchain.dev/why-langgraph-platform/', 'title': 'Why do I need LangGraph Platform for agent deployment?', 'description': 'This blog dives into technical details for why agent deployment is difficult, and how we built a platform to solve those challenges (LangGraph Platform).', 'language': 'en'}, page_content='prolonged processes. Additionally, stream endpoints can be rejoined if the connection drops, and will very soon buffer events emitted while you were disconnected.ProblemThe longer an agent runs for, the more likely it is to suffer an exception while running.How we solve itLangGraph Server implements strategies to both minimize the number of exceptions agents face (eg. by running them in workers which can use isolated event loops and background threads) as well as the amount of time needed to recover from an exception (eg. when an agent fails it is retried a configurable number of times, and each retry starts off from the most recent successful checkpoint).ProblemThe longer an agent runs for, the more you need intermediate output to be emitted while the agent is running, to show the end user that something is happening.How we solve itLangGraph Server implements multiple streaming modes, including intermediate results, token-by-token LLM messages, all the way to streaming of custom'),\n",
       " Document(metadata={'source': 'https://blog.langchain.dev/why-langgraph-platform/', 'title': 'Why do I need LangGraph Platform for agent deployment?', 'description': 'This blog dives into technical details for why agent deployment is difficult, and how we built a platform to solve those challenges (LangGraph Platform).', 'language': 'en'}, page_content='user that something is happening.How we solve itLangGraph Server implements multiple streaming modes, including intermediate results, token-by-token LLM messages, all the way to streaming of custom payloads emitted by your nodes. Streaming events are emitted in realtime from the background workers that run the agents, with minimal latency, giving you a unique combo of durable execution with realtime output. Multiple consumers can connect to the output stream of the same agent, and these connections can be re-established if dropped by the client or intermediate networking layers.Bursty AgentsProblemCertain applications, especially those with real-time user interaction, may experience \"bursty\" request loads where numerous requests hit the server simultaneously.How we solve itLangGraph Server includes a task queue, ensuring requests are handled consistently without loss, even under heavy loads. LangGraph Server is designed to scale horizontally on both the server and queue components.'),\n",
       " Document(metadata={'source': 'https://blog.langchain.dev/why-langgraph-platform/', 'title': 'Why do I need LangGraph Platform for agent deployment?', 'description': 'This blog dives into technical details for why agent deployment is difficult, and how we built a platform to solve those challenges (LangGraph Platform).', 'language': 'en'}, page_content='includes a task queue, ensuring requests are handled consistently without loss, even under heavy loads. LangGraph Server is designed to scale horizontally on both the server and queue components. This scaling is transparent to you, and doesn’t require complicated load balancing, as all server instances are stateless, and all server instances can communicate with all worker instances to support real-time streaming and cancellation. You can add as many queue instances as needed to handle your desired throughput, and they will all share the queued runs fairly, without ever executing the same run more than once.ProblemSome of types of bursty applications that are also stateful (see below) may let the user send multiple messages rapidly, without the agent having finished responding to the first one. This “double texting” can disrupt agent flows if not handled properly.How we solve itLangGraph Server offers four built-in strategies to address and manage such interactions.Stateful'),\n",
       " Document(metadata={'source': 'https://blog.langchain.dev/why-langgraph-platform/', 'title': 'Why do I need LangGraph Platform for agent deployment?', 'description': 'This blog dives into technical details for why agent deployment is difficult, and how we built a platform to solve those challenges (LangGraph Platform).', 'language': 'en'}, page_content='the first one. This “double texting” can disrupt agent flows if not handled properly.How we solve itLangGraph Server offers four built-in strategies to address and manage such interactions.Stateful AgentsProblemThere are several levels of statefulness. There is both short-term and long-term memory. Then there is human-in-the-loop (pausing at an arbitrary point in your graph until you get back human input). On top of that, there is human-on-the-loop (time travel) - letting users go back to a previous state of the graph, modify it, and resume from there.There are also different shapes of state. The most simple one is just a list of messages. Most agents, however, have more complicated state than that.The more of these stateful operations you want to support, and the more complex your state, then the more complicated it is to properly set up infrastructure to enable this.How we solve itLangGraph Platform includes optimized\\xa0checkpointers\\xa0and a\\xa0memory store, managing state across sessions'),\n",
       " Document(metadata={'source': 'https://blog.langchain.dev/why-langgraph-platform/', 'title': 'Why do I need LangGraph Platform for agent deployment?', 'description': 'This blog dives into technical details for why agent deployment is difficult, and how we built a platform to solve those challenges (LangGraph Platform).', 'language': 'en'}, page_content='then the more complicated it is to properly set up infrastructure to enable this.How we solve itLangGraph Platform includes optimized\\xa0checkpointers\\xa0and a\\xa0memory store, managing state across sessions without the need for custom solutions.LangGraph Server provides specialized endpoints for human-in-the-loop scenarios, simplifying the integration of manual oversight into agent workflows.ProblemThe more your agent is used, the more memory is used to store conversations and memories that may no longer be relevant.How we solve itLangGraph Server supports attaching TTLs (time-to-live) to both conversation threads as well as long-term memory entries, which will be cleared from memory by the platform when they expire.ConclusionBy using LangGraph Platform, you gain access to a robust, scalable deployment solution that mitigates these challenges, saving you the effort of implementing and maintaining them manually. This allows you to focus more on building effective agent behavior and less on'),\n",
       " Document(metadata={'source': 'https://blog.langchain.dev/why-langgraph-platform/', 'title': 'Why do I need LangGraph Platform for agent deployment?', 'description': 'This blog dives into technical details for why agent deployment is difficult, and how we built a platform to solve those challenges (LangGraph Platform).', 'language': 'en'}, page_content=\"solution that mitigates these challenges, saving you the effort of implementing and maintaining them manually. This allows you to focus more on building effective agent behavior and less on solving deployment infrastructure issues.To get started, read about our deployment options\\xa0here. You can also check out our announcement LangGraph Platform's general availability to learn more.\"),\n",
       " Document(metadata={'source': 'https://blog.langchain.dev/why-langgraph-platform/', 'title': 'Why do I need LangGraph Platform for agent deployment?', 'description': 'This blog dives into technical details for why agent deployment is difficult, and how we built a platform to solve those challenges (LangGraph Platform).', 'language': 'en'}, page_content='Join our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2025')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53e63686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aakas\\AppData\\Local\\Temp\\ipykernel_21688\\470793425.py:2: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = SentenceTransformerEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "embeddings = SentenceTransformerEmbeddings()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f017e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectordb = FAISS.from_documents(final_doc,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc3d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x28028834830>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32c1cc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prolonged processes. Additionally, stream endpoints can be rejoined if the connection drops, and will very soon buffer events emitted while you were disconnected.ProblemThe longer an agent runs for, the more likely it is to suffer an exception while running.How we solve itLangGraph Server implements strategies to both minimize the number of exceptions agents face (eg. by running them in workers which can use isolated event loops and background threads) as well as the amount of time needed to recover from an exception (eg. when an agent fails it is retried a configurable number of times, and each retry starts off from the most recent successful checkpoint).ProblemThe longer an agent runs for, the more you need intermediate output to be emitted while the agent is running, to show the end user that something is happening.How we solve itLangGraph Server implements multiple streaming modes, including intermediate results, token-by-token LLM messages, all the way to streaming of custom'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"The longer an agent runs, the longer it is likely to suffer an exception\"\n",
    "result = vectordb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b50f8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context\\n<context>\\n{context}\\n</context>\\n'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000028028835A30>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000028049621190>, model_name='llama-3.3-70b-versatile', temperature=0.5, model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieval Chain,Document chain\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    ")\n",
    "llm = ChatGroq(api_key=os.environ[\"GROQ_API_KEY\"],model=\"llama-3.3-70b-versatile\",temperature=0.5)\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "694c2c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You need the LangGraph Platform for agent deployment because it solves the challenges associated with deploying long-running, stateful, or bursty agents. These challenges include:\\n\\n1. Maintaining open connections for long-running agents\\n2. Handling timeouts and disruptions for prolonged processes\\n3. Minimizing exceptions and recovering from them\\n4. Emitting intermediate output for long-running agents\\n5. Handling bursty request loads\\n6. Managing stateful agents with short-term and long-term memory\\n7. Implementing human-in-the-loop and human-on-the-loop interactions\\n8. Managing memory usage for conversations and memories\\n\\nThe LangGraph Platform provides solutions to these challenges, including:\\n\\n1. Launching agent runs in the background with polling endpoints, streaming endpoints, and webhooks\\n2. Sending regular heartbeat signals to prevent connection closures\\n3. Implementing strategies to minimize exceptions and recover from them\\n4. Providing multiple streaming modes for intermediate results and custom payloads\\n5. Including a task queue to handle bursty request loads\\n6. Offering built-in strategies to manage double texting and other stateful interactions\\n7. Providing optimized checkpointers and a memory store to manage state across sessions\\n8. Supporting TTLs (time-to-live) to clear memory of irrelevant conversations and memories\\n\\nBy using the LangGraph Platform, you can focus on building effective agent behavior and less on solving deployment infrastructure issues.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain.invoke(\n",
    "    {\n",
    "        \"input\":\"The longer an agent runs, the longer it is likely to suffer an exception\",\n",
    "        \"context\":final_doc\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "144c9ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever\n",
    "\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2dee71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "915e5bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000028028834830>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context\\n<context>\\n{context}\\n</context>\\n'), additional_kwargs={})])\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000028028835A30>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000028049621190>, model_name='llama-3.3-70b-versatile', temperature=0.5, model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc8b6c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm ready to answer your question based on the provided context. What is your question?\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Get the response from the LLM\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\":\".What to do to reduce exceptions?\"})\n",
    "response['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
